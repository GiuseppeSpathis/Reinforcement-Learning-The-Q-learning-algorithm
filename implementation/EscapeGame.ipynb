{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the exercise is to run few episodes of the Q-learning\n",
        "algorithm by hand, step by step. We'll start by setting the value of the\n",
        "learning parameter Gamma = 0.8, and the initial state as Room 1."
      ],
      "metadata": {
        "id": "efxyNR09-3l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 1 Initialize the Q-table"
      ],
      "metadata": {
        "id": "L7bPMXOv-5PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 First episode"
      ],
      "metadata": {
        "id": "OY8G_x5o-8gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the second row (state 1) of matrix R. There\n",
        "are two possible actions for the current state 1: go to state 3, or\n",
        "go to state 5. By random selection, we select to go to 5 as our\n",
        "action.\n",
        "- 2.a Using Bellman equation, compute the new value of\n",
        "Q(1,5)\n",
        "- 2.b Update the Q-table accordingly.\n",
        "\n",
        "The next state, 5, now becomes the current state. Because 5 is\n",
        "the goal state, we've finished one episode. Our agent's brain now\n",
        "contains an updated matrix Q"
      ],
      "metadata": {
        "id": "-ej7cWVT_HiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 Second episode"
      ],
      "metadata": {
        "id": "AISviCi9_V27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next episode, we start with a randomly\n",
        "chosen initial state. This time, we have state 3 as our initial state\n",
        "and by random selection, we select to go to state 1 as our action.\n",
        "Update the Q(3,1) value."
      ],
      "metadata": {
        "id": "TgmSGmR4_e7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMvjraZ_-uqs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next state, 1, now becomes the current state. We repeat the\n",
        "inner loop of the Q learning algorithm because state 1 is not the\n",
        "goal state. Assume we select again randomly state 5. What is the\n",
        "new updated Q-table at the end of this episode ?"
      ],
      "metadata": {
        "id": "snf_TVjd_hrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 Try to explore more episodes to find the Q-table reaching the convergence values."
      ],
      "metadata": {
        "id": "PVYqLHoV_kIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5"
      ],
      "metadata": {
        "id": "CA0y5RT3_oDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the matrix Q gets close enough to a state of convergence,\n",
        "we know our agent has learned the most optimal paths to the goal\n",
        "state. Tracing the best sequences of states is as simple as\n",
        "following the links with the highest values at each state."
      ],
      "metadata": {
        "id": "wU65mY1S_wOK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJMKoPpp_xp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the best sequence to escape the building from room 2 ?"
      ],
      "metadata": {
        "id": "apZMSCrn_yMj"
      }
    }
  ]
}